{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-29T09:09:28.982727Z","iopub.execute_input":"2021-08-29T09:09:28.983192Z","iopub.status.idle":"2021-08-29T09:09:29.210956Z","shell.execute_reply.started":"2021-08-29T09:09:28.983010Z","shell.execute_reply":"2021-08-29T09:09:29.210262Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nips-papers/paper_authors.csv\n/kaggle/input/nips-papers/papers.csv\n/kaggle/input/nips-papers/authors.csv\n/kaggle/input/nips-papers/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"# load the dataset\ndf = pd.read_csv('/kaggle/input/nips-papers/papers.csv')\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-29T09:09:29.213163Z","iopub.execute_input":"2021-08-29T09:09:29.213421Z","iopub.status.idle":"2021-08-29T09:09:32.433958Z","shell.execute_reply.started":"2021-08-29T09:09:29.213374Z","shell.execute_reply":"2021-08-29T09:09:32.433220Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"     id  year                                              title event_type  \\\n0     1  1987  Self-Organization of Associative Database and ...        NaN   \n1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n\n                                            pdf_name          abstract  \\\n0  1-self-organization-of-associative-database-an...  Abstract Missing   \n1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n\n                                          paper_text  \n0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n3  Bayesian Query Construction for Neural\\nNetwor...  \n4  Neural Network Ensembles, Cross\\nValidation, a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>year</th>\n      <th>title</th>\n      <th>event_type</th>\n      <th>pdf_name</th>\n      <th>abstract</th>\n      <th>paper_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>1987</td>\n      <td>Self-Organization of Associative Database and ...</td>\n      <td>NaN</td>\n      <td>1-self-organization-of-associative-database-an...</td>\n      <td>Abstract Missing</td>\n      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>10</td>\n      <td>1987</td>\n      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n      <td>NaN</td>\n      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n      <td>Abstract Missing</td>\n      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>100</td>\n      <td>1988</td>\n      <td>Storing Covariance by the Associative Long-Ter...</td>\n      <td>NaN</td>\n      <td>100-storing-covariance-by-the-associative-long...</td>\n      <td>Abstract Missing</td>\n      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1000</td>\n      <td>1994</td>\n      <td>Bayesian Query Construction for Neural Network...</td>\n      <td>NaN</td>\n      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n      <td>Abstract Missing</td>\n      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1001</td>\n      <td>1994</td>\n      <td>Neural Network Ensembles, Cross Validation, an...</td>\n      <td>NaN</td>\n      <td>1001-neural-network-ensembles-cross-validation...</td>\n      <td>Abstract Missing</td>\n      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:32.435283Z","iopub.execute_input":"2021-08-29T09:09:32.435581Z","iopub.status.idle":"2021-08-29T09:09:32.450629Z","shell.execute_reply.started":"2021-08-29T09:09:32.435535Z","shell.execute_reply":"2021-08-29T09:09:32.449447Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7241 entries, 0 to 7240\nData columns (total 7 columns):\nid            7241 non-null int64\nyear          7241 non-null int64\ntitle         7241 non-null object\nevent_type    2422 non-null object\npdf_name      7241 non-null object\nabstract      7241 non-null object\npaper_text    7241 non-null object\ndtypes: int64(2), object(5)\nmemory usage: 396.1+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"{} abstracts are missing\".format(df[df['abstract']=='Abstract Missing']['abstract'].count()))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:32.452463Z","iopub.execute_input":"2021-08-29T09:09:32.452861Z","iopub.status.idle":"2021-08-29T09:09:32.463382Z","shell.execute_reply.started":"2021-08-29T09:09:32.452696Z","shell.execute_reply":"2021-08-29T09:09:32.462442Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"3317 abstracts are missing\n","output_type":"stream"}]},{"cell_type":"code","source":"import pprint\nsample = 941\npprint.pprint(\"TITLE:{}\".format(df['title'][sample]))\npprint.pprint(\"ABSTRACT:{}\".format(df['abstract'][sample]))\npprint.pprint(\"FULL TEXT:{}\".format(df['paper_text'][sample][:1000]))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:32.468001Z","iopub.execute_input":"2021-08-29T09:09:32.468632Z","iopub.status.idle":"2021-08-29T09:09:32.481711Z","shell.execute_reply.started":"2021-08-29T09:09:32.468432Z","shell.execute_reply":"2021-08-29T09:09:32.480951Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"'TITLE:Algorithms for Non-negative Matrix Factorization'\n('ABSTRACT:Non-negative matrix factorization (NMF) has previously been shown '\n 'to \\r\\n'\n 'be a useful decomposition for multivariate data. Two different multi- \\r\\n'\n 'plicative algorithms for NMF are analyzed. They differ only slightly in \\r\\n'\n 'the multiplicative factor used in the update rules. One algorithm can be \\r\\n'\n 'shown to minimize the conventional least squares error while the other \\r\\n'\n 'minimizes the generalized Kullback-Leibler divergence. The monotonic \\r\\n'\n 'convergence of both algorithms can be proven using an auxiliary func- \\r\\n'\n 'tion analogous to that used for proving convergence of the Expectation- \\r\\n'\n 'Maximization algorithm. The algorithms can also be interpreted as diag- \\r\\n'\n 'onally rescaled gradient descent, where the rescaling factor is '\n 'optimally \\r\\n'\n 'chosen to ensure convergence. ')\n('FULL TEXT:Algorithms for Non-negative Matrix\\n'\n 'Factorization\\n'\n '\\n'\n 'Daniel D. Lee*\\n'\n '*BelJ Laboratories\\n'\n 'Lucent Technologies\\n'\n 'Murray Hill, NJ 07974\\n'\n '\\n'\n 'H. Sebastian Seung*t\\n'\n 'tDept. of Brain and Cog. Sci.\\n'\n 'Massachusetts Institute of Technology\\n'\n 'Cambridge, MA 02138\\n'\n '\\n'\n 'Abstract\\n'\n 'Non-negative matrix factorization (NMF) has previously been shown to\\n'\n 'be a useful decomposition for multivariate data. Two different '\n 'multiplicative algorithms for NMF are analyzed. They differ only slightly '\n 'in\\n'\n 'the multiplicative factor used in the update rules. One algorithm can be\\n'\n 'shown to minimize the conventional least squares error while the other\\n'\n 'minimizes the generalized Kullback-Leibler divergence. The monotonic\\n'\n 'convergence of both algorithms can be proven using an auxiliary function '\n 'analogous to that used for proving convergence of the '\n 'ExpectationMaximization algorithm. The algorithms can also be interpreted as '\n 'diagonally rescaled gradient descent, where the rescaling factor is '\n 'optimally\\n'\n 'chosen to ensure convergence.\\n'\n '\\n'\n '1 Introduction\\n'\n 'Unsu')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This dataset contains 7 columns: id, year, title, even_type, pdf_name, abstract and paper_text. We are mostly interested in the paper_text which include both title and abstract.","metadata":{}},{"cell_type":"markdown","source":"## Pre-processing","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\n##Creating a list of custom stopwords\nnew_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n             \"show\", \"result\", \"large\", \n             \"also\", \"one\", \"two\", \"three\", \n             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\nstop_words = list(stop_words.union(new_words))\n\ndef pre_process(text):\n    \n    # lowercase\n    text=text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    # remove stopwords\n    text = [word for word in text if word not in stop_words]\n\n    # remove words less than three letters\n    text = [word for word in text if len(word) >= 3]\n\n    # lemmatize\n    lmtzr = WordNetLemmatizer()\n    text = [lmtzr.lemmatize(word) for word in text]\n    \n    return ' '.join(text)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:32.487926Z","iopub.execute_input":"2021-08-29T09:09:32.488194Z","iopub.status.idle":"2021-08-29T09:09:34.060851Z","shell.execute_reply.started":"2021-08-29T09:09:32.488147Z","shell.execute_reply":"2021-08-29T09:09:34.060084Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\ndocs = df['paper_text'].apply(lambda x:pre_process(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:34.062379Z","iopub.execute_input":"2021-08-29T09:09:34.062685Z","iopub.status.idle":"2021-08-29T09:13:06.488268Z","shell.execute_reply.started":"2021-08-29T09:09:34.062638Z","shell.execute_reply":"2021-08-29T09:13:06.487231Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 3min 31s, sys: 324 ms, total: 3min 32s\nWall time: 3min 32s\n","output_type":"stream"}]},{"cell_type":"code","source":"docs[1][0:103]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:13:06.489873Z","iopub.execute_input":"2021-08-29T09:13:06.490416Z","iopub.status.idle":"2021-08-29T09:13:06.497123Z","shell.execute_reply.started":"2021-08-29T09:13:06.490134Z","shell.execute_reply":"2021-08-29T09:13:06.496352Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'mean field theory layer visual cortex application artificial neural network christopher scofield center'"},"metadata":{}}]},{"cell_type":"markdown","source":"## 1.TF-IDF and Scikit-learn\n\nBased on the tutorial of [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb)\n\nTF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score.\n\n### 1.1 CountVectorizer to create a vocabulary and generate word counts","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import CountVectorizer\n#docs = docs.tolist()\n#create a vocabulary of words, \ncv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n                   max_features=10000,  # the size of the vocabulary\n                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n                  )\nword_count_vector=cv.fit_transform(docs)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:13:06.498945Z","iopub.execute_input":"2021-08-29T09:13:06.499574Z","iopub.status.idle":"2021-08-29T09:15:56.412999Z","shell.execute_reply.started":"2021-08-29T09:13:06.499297Z","shell.execute_reply":"2021-08-29T09:15:56.412292Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 2min 45s, sys: 4.7 s, total: 2min 49s\nWall time: 2min 49s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1.2 TfidfTransformer to Compute Inverse Document Frequency (IDF)","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:56.414432Z","iopub.execute_input":"2021-08-29T09:15:56.414727Z","iopub.status.idle":"2021-08-29T09:15:56.493985Z","shell.execute_reply.started":"2021-08-29T09:15:56.414681Z","shell.execute_reply":"2021-08-29T09:15:56.493136Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CPU times: user 23.3 ms, sys: 21 ms, total: 44.4 ms\nWall time: 70.4 ms\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top keywords.","metadata":{}},{"cell_type":"code","source":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:56.495746Z","iopub.execute_input":"2021-08-29T09:15:56.496295Z","iopub.status.idle":"2021-08-29T09:15:56.506650Z","shell.execute_reply.started":"2021-08-29T09:15:56.496008Z","shell.execute_reply":"2021-08-29T09:15:56.505733Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# get feature names\nfeature_names=cv.get_feature_names()\n\ndef get_keywords(idx, docs):\n\n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n\n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n\n    #extract only the top n; n here is 10\n    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n    \n    return keywords\n\ndef print_results(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k,keywords[k])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:56.508164Z","iopub.execute_input":"2021-08-29T09:15:56.508935Z","iopub.status.idle":"2021-08-29T09:15:56.563979Z","shell.execute_reply.started":"2021-08-29T09:15:56.508516Z","shell.execute_reply":"2021-08-29T09:15:56.563301Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"idx=941\nkeywords=get_keywords(idx, docs)\nprint_results(idx,keywords, df)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:56.565497Z","iopub.execute_input":"2021-08-29T09:15:56.565958Z","iopub.status.idle":"2021-08-29T09:15:56.591243Z","shell.execute_reply.started":"2021-08-29T09:15:56.565766Z","shell.execute_reply":"2021-08-29T09:15:56.590518Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\n=====Title=====\nAlgorithms for Non-negative Matrix Factorization\n\n=====Abstract=====\nNon-negative matrix factorization (NMF) has previously been shown to \nbe a useful decomposition for multivariate data. Two different multi- \nplicative algorithms for NMF are analyzed. They differ only slightly in \nthe multiplicative factor used in the update rules. One algorithm can be \nshown to minimize the conventional least squares error while the other \nminimizes the generalized Kullback-Leibler divergence. The monotonic \nconvergence of both algorithms can be proven using an auxiliary func- \ntion analogous to that used for proving convergence of the Expectation- \nMaximization algorithm. The algorithms can also be interpreted as diag- \nonally rescaled gradient descent, where the rescaling factor is optimally \nchosen to ensure convergence. \n\n===Keywords===\nupdate rule 0.344\nupdate 0.285\nauxiliary 0.212\nnon negative matrix 0.21\nnegative matrix 0.209\nrule 0.192\nnmf 0.183\nmultiplicative 0.175\nmatrix factorization 0.163\nmatrix 0.163\n","output_type":"stream"}]},{"cell_type":"markdown","source":" For instance, non-negative matrix factorization meets us 5 time: non negative matrix, negative matrix, nmf, matrix factorization, matrix. Adding a 4-grams does not change the situation. Similar keywords appears due to the fact that TF-IDF does not take into account the context, the keywords importance comes only from their frequencies relationship. Thus, TF-IDF is a quick, intuitive, but not the best way to extract keywords from the text. Let's look at other ways.","metadata":{}},{"cell_type":"markdown","source":"## 2. Gensim implementation of TextRank summarization algorithm\n\nGensim is a free Python library designed to automatically extract semantic topics from documents. The gensim implementation is based on the popular TextRank algorithm. \n\n[Documentation](https://radimrehurek.com/gensim/summarization/keywords.html)\n\n[Tutorial](https://rare-technologies.com/text-summarization-with-gensim/)","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Small text","metadata":{}},{"cell_type":"code","source":"import gensim\ntext = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n\"minimize the conventional least squares error while the other minimizes the  \" + \\\n\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n\"rescaling factor is optimally chosen to ensure convergence.\"\ngensim.summarization.keywords(text, \n         ratio=0.5,               # use 50% of original text\n         words=None,              # Number of returned words\n         split=True,              # Whether split keywords\n         scores=False,            # Whether score of keyword\n         pos_filter=('NN', 'JJ'), # Part of speech (nouns, adjectives etc.) filters\n         lemmatize=True,         # If True - lemmatize words\n         deacc=True)              # If True - remove accentuation","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:56.594328Z","iopub.execute_input":"2021-08-29T09:15:56.594562Z","iopub.status.idle":"2021-08-29T09:15:57.059684Z","shell.execute_reply.started":"2021-08-29T09:15:56.594521Z","shell.execute_reply":"2021-08-29T09:15:57.058916Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['factor',\n 'convergence',\n 'rescaling',\n 'multiplicative',\n 'function',\n 'kullback',\n 'gradient',\n 'algorithm',\n 'matrix',\n 'useful decomposition',\n 'multivariate',\n 'data',\n 'squares']"},"metadata":{}}]},{"cell_type":"code","source":"print(\"SUMMARY: \", gensim.summarization.summarize(text,\n                                                  ratio = 0.5,\n                                                  split = True))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:57.062933Z","iopub.execute_input":"2021-08-29T09:15:57.063180Z","iopub.status.idle":"2021-08-29T09:15:57.076572Z","shell.execute_reply.started":"2021-08-29T09:15:57.063133Z","shell.execute_reply":"2021-08-29T09:15:57.075234Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"SUMMARY:  ['Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data.', 'Two different multiplicative algorithms for NMF are analyzed.', 'They differ only slightly in the multiplicative factor used in the update rules.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2.2 Large text","metadata":{}},{"cell_type":"code","source":"def get_keywords_gensim(idx, docs):\n    \n    keywords=gensim.summarization.keywords(docs[idx], \n                                  ratio=None, \n                                  words=10,         \n                                  split=True,             \n                                  scores=False,           \n                                  pos_filter=None, \n                                  lemmatize=True,         \n                                  deacc=True)              \n    \n    return keywords\n\ndef print_results_gensim(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:57.078454Z","iopub.execute_input":"2021-08-29T09:15:57.078770Z","iopub.status.idle":"2021-08-29T09:15:57.086086Z","shell.execute_reply.started":"2021-08-29T09:15:57.078710Z","shell.execute_reply":"2021-08-29T09:15:57.084990Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"idx=941\nkeywords=get_keywords_gensim(idx, docs)\nprint_results_gensim(idx,keywords, df)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:57.087563Z","iopub.execute_input":"2021-08-29T09:15:57.087977Z","iopub.status.idle":"2021-08-29T09:15:57.203677Z","shell.execute_reply.started":"2021-08-29T09:15:57.087868Z","shell.execute_reply":"2021-08-29T09:15:57.202920Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n=====Title=====\nAlgorithms for Non-negative Matrix Factorization\n\n=====Abstract=====\nNon-negative matrix factorization (NMF) has previously been shown to \nbe a useful decomposition for multivariate data. Two different multi- \nplicative algorithms for NMF are analyzed. They differ only slightly in \nthe multiplicative factor used in the update rules. One algorithm can be \nshown to minimize the conventional least squares error while the other \nminimizes the generalized Kullback-Leibler divergence. The monotonic \nconvergence of both algorithms can be proven using an auxiliary func- \ntion analogous to that used for proving convergence of the Expectation- \nMaximization algorithm. The algorithms can also be interpreted as diag- \nonally rescaled gradient descent, where the rescaling factor is optimally \nchosen to ensure convergence. \n\n===Keywords===\nfactorized\nalgorithm\nmatrix\nupdate rule\nfunction\ndata\nconverge\ntheorem\ngradient\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The keywords highlight the main point , but still miss valuable information","metadata":{}},{"cell_type":"markdown","source":"## 3. Yet Another Keyword Extractor (Yake)\n\n[Documentation](https://github.com/LIAAD/yake)","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/LIAAD/yake","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:57.206460Z","iopub.execute_input":"2021-08-29T09:15:57.206686Z","iopub.status.idle":"2021-08-29T09:16:12.733736Z","shell.execute_reply.started":"2021-08-29T09:15:57.206646Z","shell.execute_reply":"2021-08-29T09:16:12.732936Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/LIAAD/yake\n  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-9y4p6o_k\n  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-9y4p6o_k\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.6/site-packages/tabulate-0.8.5-py3.6.egg (from yake==0.4.8) (0.8.5)\nRequirement already satisfied: click>=6.0 in /opt/conda/lib/python3.6/site-packages (from yake==0.4.8) (7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from yake==0.4.8) (1.16.4)\nCollecting segtok (from yake==0.4.8)\n  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\nRequirement already satisfied: networkx in /opt/conda/lib/python3.6/site-packages (from yake==0.4.8) (2.3)\nCollecting jellyfish (from yake==0.4.8)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/88/e6eba0ebd8a11eb0a03392d827f0a605ad45fbb24234f7db98ca1ecb41b2/jellyfish-0.8.8.tar.gz (134kB)\n\u001b[K     |████████████████████████████████| 143kB 6.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from segtok->yake==0.4.8) (2019.8.19)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx->yake==0.4.8) (4.4.0)\nBuilding wheels for collected packages: yake, segtok, jellyfish\n  Building wheel for yake (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for yake: filename=yake-0.4.8-py2.py3-none-any.whl size=60162 sha256=5c8d25bc3b1e075fd08f7c9556f6c7393b0a1c5ab95739f46bc23e1daa262cef\n  Stored in directory: /tmp/pip-ephem-wheel-cache-sxw5vvxj/wheels/be/35/27/e4ebd54b78c1806ed8b0271ce247fcd91e2bedde35889fbc9b\n  Building wheel for segtok (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=4e8599d97019c168609cb3148efc297e311f345c58c7f71987aa71b0a867c745\n  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n  Building wheel for jellyfish (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for jellyfish: filename=jellyfish-0.8.8-cp36-cp36m-linux_x86_64.whl size=86530 sha256=e8d083c6499df8119e2dc32e019e46abc69ef0438f4990de28b95570f894dd51\n  Stored in directory: /root/.cache/pip/wheels/5d/aa/7d/3ad7b78c3b723e5c6f3febc0cc2eca249716c3e09e16993240\nSuccessfully built yake segtok jellyfish\nInstalling collected packages: segtok, jellyfish, yake\nSuccessfully installed jellyfish-0.8.8 segtok-1.5.10 yake-0.4.8\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_results(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:16:12.737663Z","iopub.execute_input":"2021-08-29T09:16:12.737922Z","iopub.status.idle":"2021-08-29T09:16:12.743085Z","shell.execute_reply.started":"2021-08-29T09:16:12.737877Z","shell.execute_reply":"2021-08-29T09:16:12.742260Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import yake\n\ndef get_keywords_yake(idx, docs):\n    y = yake.KeywordExtractor(lan='en',          # language\n                             n = 3,              # n-gram size\n                             dedupLim = 0.9,     # deduplicationthresold\n                             dedupFunc = 'seqm', #  deduplication algorithm\n                             windowsSize = 1,\n                             top = 10,           # number of keys\n                             features=None)           \n    \n    keywords = y.extract_keywords(text)\n    return keywords\n\nidx=941\nkeywords = get_keywords_yake(idx, docs[idx])\nprint_results(idx, keywords, df)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:16:12.744296Z","iopub.execute_input":"2021-08-29T09:16:12.744597Z","iopub.status.idle":"2021-08-29T09:16:13.201727Z","shell.execute_reply.started":"2021-08-29T09:16:12.744552Z","shell.execute_reply":"2021-08-29T09:16:13.201051Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\n=====Title=====\nAlgorithms for Non-negative Matrix Factorization\n\n=====Abstract=====\nNon-negative matrix factorization (NMF) has previously been shown to \nbe a useful decomposition for multivariate data. Two different multi- \nplicative algorithms for NMF are analyzed. They differ only slightly in \nthe multiplicative factor used in the update rules. One algorithm can be \nshown to minimize the conventional least squares error while the other \nminimizes the generalized Kullback-Leibler divergence. The monotonic \nconvergence of both algorithms can be proven using an auxiliary func- \ntion analogous to that used for proving convergence of the Expectation- \nMaximization algorithm. The algorithms can also be interpreted as diag- \nonally rescaled gradient descent, where the rescaling factor is optimally \nchosen to ensure convergence. \n\n===Keywords===\n('Non-negative matrix factorization', 0.0041066275750552455)\n('Non-negative matrix', 0.026529705128479162)\n('matrix factorization', 0.026529705128479162)\n('multivariate data', 0.026529705128479162)\n('decomposition for multivariate', 0.03127464030655176)\n('NMF', 0.06699743201311577)\n('NMF are analyzed', 0.11148839518508058)\n('algorithms', 0.13323194577601624)\n('previously been shown', 0.1372005684192386)\n('Non-negative', 0.1484061535685674)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Key phrases are repeated, and the text needs pre-processing to remove stop words","metadata":{}},{"cell_type":"markdown","source":"## 4. Keyphrases extraction using pke\n\n`pke` an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\n\n`pke` currently implements the following keyphrase extraction models:\n\n* Unsupervised models\n  * Statistical models\n    * TfIdf [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#tfidf)]\n    * KPMiner [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#kpminer), [article by (El-Beltagy and Rafea, 2010)](http://www.aclweb.org/anthology/S10-1041.pdf)]\n    * YAKE [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#yake), [article by (Campos et al., 2020)](https://doi.org/10.1016/j.ins.2019.09.013)]\n  * Graph-based models\n    * TextRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#textrank), [article by (Mihalcea and Tarau, 2004)](http://www.aclweb.org/anthology/W04-3252.pdf)]\n    * SingleRank  [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#singlerank), [article by (Wan and Xiao, 2008)](http://www.aclweb.org/anthology/C08-1122.pdf)]\n    * TopicRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#topicrank), [article by (Bougouin et al., 2013)](http://aclweb.org/anthology/I13-1062.pdf)]\n    * TopicalPageRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#topicalpagerank), [article by (Sterckx et al., 2015)](http://users.intec.ugent.be/cdvelder/papers/2015/sterckx2015wwwb.pdf)]\n    * PositionRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#positionrank), [article by (Florescu and Caragea, 2017)](http://www.aclweb.org/anthology/P17-1102.pdf)]\n    * MultipartiteRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#multipartiterank), [article by (Boudin, 2018)](https://arxiv.org/abs/1803.08721)]\n* Supervised models\n  * Feature-based models\n    * Kea [[documentation](https://boudinfl.github.io/pke/build/html/supervised.html#kea), [article by (Witten et al., 2005)](https://www.cs.waikato.ac.nz/ml/publications/2005/chap_Witten-et-al_Windows.pdf)]\n    * WINGNUS [[documentation](https://boudinfl.github.io/pke/build/html/supervised.html#wingnus), [article by (Nguyen and Luong, 2010)](http://www.aclweb.org/anthology/S10-1035.pdf)]\n","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/boudinfl/pke.git","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:16:13.203092Z","iopub.execute_input":"2021-08-29T09:16:13.203423Z","iopub.status.idle":"2021-08-29T09:16:22.459708Z","shell.execute_reply.started":"2021-08-29T09:16:13.203377Z","shell.execute_reply":"2021-08-29T09:16:22.458801Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/boudinfl/pke.git\n  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-g5apznug\n  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-g5apznug\nRequirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (3.2.4)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (2.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (1.16.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (1.2.1)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (2.1.8)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (1.12.0)\nRequirement already satisfied: sklearn in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (0.0)\nRequirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (1.1.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (0.17.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from pke==1.8.1) (0.13.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx->pke==1.8.1) (4.4.0)\nRequirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (0.9.6)\nRequirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (2.0.1)\nRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (0.1.0)\nRequirement already satisfied: thinc<7.1.0,>=7.0.8 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (7.0.8)\nRequirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (0.2.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (2.22.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (2.0.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (1.0.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy->pke==1.8.1) (0.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn->pke==1.8.1) (0.21.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.8->spacy->pke==1.8.1) (4.36.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2019.9.11)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (1.24.2)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2.8)\nBuilding wheels for collected packages: pke\n  Building wheel for pke (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pke: filename=pke-1.8.1-cp36-none-any.whl size=8764021 sha256=d894f0c9441150d05a71dc3feb676f783e657e4b9af4b3ee91799e3f360d6544\n  Stored in directory: /tmp/pip-ephem-wheel-cache-v9ftfral/wheels/8d/24/54/6582e854e9e32dd6c632af6762b3a5d2f6b181c2992e165462\nSuccessfully built pke\nInstalling collected packages: pke\nSuccessfully installed pke-1.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pke","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:16:22.461327Z","iopub.execute_input":"2021-08-29T09:16:22.461656Z","iopub.status.idle":"2021-08-29T09:16:25.995832Z","shell.execute_reply.started":"2021-08-29T09:16:22.461608Z","shell.execute_reply":"2021-08-29T09:16:25.994884Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### 5.1  SingleRank\n\nThis model is an extension of the TextRank model that uses the number of co-occurrences to weigh edges in the graph.","metadata":{}},{"cell_type":"code","source":"# define the set of valid Part-of-Speeches\npos = {'NOUN', 'PROPN', 'ADJ'}\n\n# 1. create a SingleRank extractor.\nextractor = pke.unsupervised.SingleRank()\n\n# 2. load the content of the document.\nextractor.load_document(input=text,\n                        language='en',\n                        normalization=None)\n\n# 3. select the longest sequences of nouns and adjectives as candidates.\nextractor.candidate_selection(pos=pos)\n\n# 4. weight the candidates using the sum of their word's scores that are\n#    computed using random walk. In the graph, nodes are words of\n#    certain part-of-speech (nouns and adjectives) that are connected if\n#    they occur in a window of 10 words.\nextractor.candidate_weighting(window=10,\n                              pos=pos)\n\n# 5. get the 10-highest scored candidates as keyphrases\nkeyphrases = extractor.get_n_best(n=10)\n\nidx = 941\n# now print the results\nprint(\"\\n=====Title=====\")\nprint(df['title'][idx])\nprint(\"\\n=====Abstract=====\")\nprint(df['abstract'][idx])\nprint(\"\\n===Keywords===\")\nfor k in keyphrases:\n    print(k[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:16:25.997242Z","iopub.execute_input":"2021-08-29T09:16:25.997550Z","iopub.status.idle":"2021-08-29T09:16:26.831576Z","shell.execute_reply.started":"2021-08-29T09:16:25.997505Z","shell.execute_reply":"2021-08-29T09:16:26.830862Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\n=====Title=====\nAlgorithms for Non-negative Matrix Factorization\n\n=====Abstract=====\nNon-negative matrix factorization (NMF) has previously been shown to \nbe a useful decomposition for multivariate data. Two different multi- \nplicative algorithms for NMF are analyzed. They differ only slightly in \nthe multiplicative factor used in the update rules. One algorithm can be \nshown to minimize the conventional least squares error while the other \nminimizes the generalized Kullback-Leibler divergence. The monotonic \nconvergence of both algorithms can be proven using an auxiliary func- \ntion analogous to that used for proving convergence of the Expectation- \nMaximization algorithm. The algorithms can also be interpreted as diag- \nonally rescaled gradient descent, where the rescaling factor is optimally \nchosen to ensure convergence. \n\n===Keywords===\ndifferent multiplicative algorithms\nnon - negative matrix factorization\nconventional least squares error\nmultiplicative factor\nmonotonic convergence\nalgorithms\nmaximization algorithm\nauxiliary function analogous\nrescaling factor\nconvergence\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 5.2 TopicRank","metadata":{}},{"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\n\n# 1. create a TopicRank extractor.\nextractor = pke.unsupervised.TopicRank()\n\n# 2. load the content of the document.\nextractor.load_document(input=text)\n\n# 3. select the longest sequences of nouns and adjectives, that do\n#    not contain punctuation marks or stopwords as candidates.\npos = {'NOUN', 'PROPN', 'ADJ'}\nstoplist = list(string.punctuation)\nstoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\nstoplist += stopwords.words('english')\nextractor.candidate_selection(pos=pos, stoplist=stoplist)\n\n# 4. build topics by grouping candidates with HAC (average linkage,\n#    threshold of 1/4 of shared stems). Weight the topics using random\n#    walk, and select the first occuring candidate from each topic.\nextractor.candidate_weighting(threshold=0.74, method='average')\n\n# 5. get the 10-highest scored candidates as keyphrases\nkeyphrases = extractor.get_n_best(n=10)\n\nidx = 941\n# now print the results\nprint(\"\\n=====Title=====\")\nprint(df['title'][idx])\nprint(\"\\n=====Abstract=====\")\nprint(df['abstract'][idx])\nprint(\"\\n===Keywords===\")\nfor k in keyphrases:\n    print(k[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:16:26.834620Z","iopub.execute_input":"2021-08-29T09:16:26.834862Z","iopub.status.idle":"2021-08-29T09:16:27.088375Z","shell.execute_reply.started":"2021-08-29T09:16:26.834818Z","shell.execute_reply":"2021-08-29T09:16:27.087660Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\n=====Title=====\nAlgorithms for Non-negative Matrix Factorization\n\n=====Abstract=====\nNon-negative matrix factorization (NMF) has previously been shown to \nbe a useful decomposition for multivariate data. Two different multi- \nplicative algorithms for NMF are analyzed. They differ only slightly in \nthe multiplicative factor used in the update rules. One algorithm can be \nshown to minimize the conventional least squares error while the other \nminimizes the generalized Kullback-Leibler divergence. The monotonic \nconvergence of both algorithms can be proven using an auxiliary func- \ntion analogous to that used for proving convergence of the Expectation- \nMaximization algorithm. The algorithms can also be interpreted as diag- \nonally rescaled gradient descent, where the rescaling factor is optimally \nchosen to ensure convergence. \n\n===Keywords===\ndifferent multiplicative algorithms\nmonotonic convergence\nmultiplicative factor\nnmf\nleibler divergence\nexpectation\nmultivariate data\nkullback\nuseful decomposition\nupdate rules\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}